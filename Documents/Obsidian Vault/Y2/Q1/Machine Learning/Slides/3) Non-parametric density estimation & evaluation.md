*Learning goals*
- explain how you obtain a classifier using a Gaussian (multivariate) distribution for each class
- implement a simple univariate classifier in Python
- explain what the 'curse of dimensionality' is
- explain the advantages and disadvantages are of the Quadratic classifier, the LDA and the nearest mean classifier
- identify when scaling of the features is important and how to cope with feature scaling
## Distances
### Manhattan distance
- D(a,b) = for each feature: |a - b|
### Hamming
- D(a,b) = for each feature: count a != b
### Euclidian distance
- D(a,b) = sqrt(for each feature(a-b)^2) 
## Classifier evaluation
![[Screenshot 2025-07-12 152409.png]]

| Classifier                                                                                                                                                                                                                                                                                                                                                                                                                                            | Use case (compared to previous)                                                                                                                                                                     | Pros                                                                                                 | Cons                                                                                                                                                                                                                                                                                                                                                                                                  | Choosing parameters                                                 |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------- |
| QDA                                                                                                                                                                                                                                                                                                                                                                                                                                                   | Curved boundaries                                                                                                                                                                                   |                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                     |
| LDA                                                                                                                                                                                                                                                                                                                                                                                                                                                   | Straight lines                                                                                                                                                                                      |                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                     |
| Nearest mean                                                                                                                                                                                                                                                                                                                                                                                                                                          | Similar class covariances                                                                                                                                                                           |                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                     |
| Histogram                                                                                                                                                                                                                                                                                                                                                                                                                                             | Defined bins/intervals & locations                                                                                                                                                                  |                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                     |
| k-nearest neighbour: most common class of k nearest neighbours<br>![[Pasted image 20250915092221.png]]<br>- ni is total amount of elements of class i <br>- ki is amount of elements of class i within Vk<br>- Vk is volume (M-dimensional) which fits k closest elements                                                                                                                                                                             | Non linear boundaries  and no training phase                                                                                                                                                        | 1) Easy to implement<br>2) Ability to model complex relationships => good classification performance | 1) - k: how many neighbours to take into account<br>- Large: most probable class at location (aka Bayes)<br>- Small: unstable (adding 1 point in training data can radically change classification) => large training set needed<br>2) Feature scaling is expensive<br>3) Complete training set required (distances computed on the fly)<br><br>Note: pay attention to axes (both must scale equally) | Vary k and test on dataset, pick k with lowest classification error |
| Parzen/KDE: for each point, put a function (e.g. Gaussian) at point, add the individual functions. Do this for each of the feature values of each class (results in M-dimensional gaussian for M features). Plug in feature values, and see what class is most likely.<br>![[Pasted image 20250915090525.png]]<br>- h is size/width of kernel<br><br>KDE different than histogram since KDE is centred at point whereas histogram has fixed intervals | Complex/ multimodel distributions (multiple peaks)                                                                                                                                                  |                                                                                                      | ![[Screenshot 2025-07-06 115043 2.png]]<br>- h: how spread the probability based on distances<br>- Bottom left: too spikey and too narrow contours => overfitting due to small h<br>- Bottom right: overtly smooth lines => underfitting due to large h<br>- Top: optimal classification: important features captured and moderate distance between contours (medium h)                               |                                                                     |
| Naive bayes: features are conditionally independent. For each class, what is probability of observing each feature value. Given new data point and its features, what class is most likely<br>![[Pasted image 20250915101137.png]]                                                                                                                                                                                                                    | Non linear boundaries, large datasets, features are independent and fast predictions<br><br>Doesn't take correlated features into account                                                           |                                                                                                      | Does not work for correlated variables<br>![[Screenshot 2025-07-12 155310.png]]<br><br>Compensate for 0/2 (multiplying features would result in 0) => add a base number<br><br><mark style="background: #A33B20;">Can be fooled by adding a lot of valid words</mark>                                                                                                                                 |                                                                     |
| Logistic classifier: probability of yes/no                                                                                                                                                                                                                                                                                                                                                                                                            | 1 or 2 class problem (nothing about types of boundaries)                                                                                                                                            |                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                     |
| Support vector machine: hyperplane (boundary) separates 2 classes. The distance between the hyperplane and the surrounding points it maximized, the points that touch the margin are called support vectors. Hyperplane is exactly in middle (margins on both sides must be equal)<br>![[Pasted image 20250712093539.png]]                                                                                                                            | 1) High dimensionality, since less influence of the curse of dimensionality, because only look at points closest to boundary<br>2) Clearly separated margins make it easier to construct hyperplane |                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                     |
| Decision trees                                                                                                                                                                                                                                                                                                                                                                                                                                        | If then cases, non linear boundaries or mixed data types                                                                                                                                            |                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                     |
| Perceptron (aka artificial neuron): given an input, a 2 class boundary and bias, decide which side the input is on                                                                                                                                                                                                                                                                                                                                    | \<almost solely used in neural networks>                                                                                                                                                            |                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                     |
| Neural networks: bunch of perceptrons                                                                                                                                                                                                                                                                                                                                                                                                                 | Complex, non-linear boundaries, large datasets                                                                                                                                                      |                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                     |

### Evaluation methods
- Use cases: high dimensional data that we can't visualize or hyperparameters (fixed parameters such as h in Parzen)
	- These hyperparameters must be decided before seeing the training set, otherwise, cheating
#### Train vs test set
![[Screenshot 2025-07-06 120300.png]]![[Screenshot 2025-07-06 120623.png]]
- Derived by fact that error e^ is sum of Bernoulli RV:
	![[Screenshot 2025-07-06 120704.png]]
##### Dataset split
![[Screenshot 2025-07-06 131226.png]]
##### Splitting sets
![[Screenshot 2025-07-06 131429.png]]
![[Screenshot 2025-07-06 131452.png]]
-  For every set, train model on rest of data points, test singular (repeat k times)
![[Screenshot 2025-07-06 131731.png]]
- For every data point, train model on rest of data points, test singular point (repeat N tmies)
![[Screenshot 2025-07-06 132331.png]]
- For every dataset, consider it a test set, loop over all other datasets, consider them training sets and compute the error with the h of the training set for the test set
<mark style="background: #A33B20;">- NEEDES DOUBLE CHECKING Solves data leakage: model has seen data during testing => optimised hyper parameters</mark>
###### Effectiveness
![[Screenshot 2025-07-06 133747.png]]![[Screenshot 2025-07-06 134114.png]]
##### Classifier complexity
![[Screenshot 2025-07-06 134719.png]]![[Screenshot 2025-07-06 140407.png]]
##### Bias-variance trade-off
- Complex models: high variance, low bias but require more data
- Simple models: low variance, high bias and very little data required
###### Expected error
![[Screenshot 2025-07-06 141730.png]]
![[Screenshot 2025-07-06 142028.png]]
### Metrics
![[Screenshot 2025-07-06 142628.png]]
#### Confusion matrix
![[Screenshot 2025-07-06 142942.png]]![[Screenshot 2025-07-06 143116.png]]
#### Two class problem error rate
![[Screenshot 2025-07-06 143456.png]]
- In the image, we see how changing priors influences the error rates
#### ROC curve
Example:
![[Screenshot 2025-07-06 145149.png]]![[Screenshot 2025-07-06 145205.png]]
